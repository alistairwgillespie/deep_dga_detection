{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Toolkit for Splunk - Barebone Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dga'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b541f0ef203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdga\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatsnu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# from __future__ import unicode_literals, print_function, division\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dga'"
     ]
    }
   ],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "from io import open\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from dga import matsnu\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -la \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"PyTorch: \" + torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer mode=stage algo=barebone epochs=10 batch_size=1 s from feature_* into app:barebone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/alexa_top_1m.csv' does not exist: b'data/alexa_top_1m.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ade33b32545b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m alexa_df = pd.read_csv(\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alexa_top_1m.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ).iloc[:10000]\n\u001b[1;32m      6\u001b[0m \u001b[0malexa_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'benign'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/alexa_top_1m.csv' does not exist: b'data/alexa_top_1m.csv'"
     ]
    }
   ],
   "source": [
    "# Training dataframe to csv\n",
    "alexa_df = pd.read_csv(\n",
    "    os.path.join(data_dir, 'alexa_top_1m.csv'),\n",
    "    names=['domain']\n",
    ").iloc[:10000]\n",
    "alexa_df['label'] = 'benign'\n",
    "\n",
    "matsnu_df = pd.DataFrame(\n",
    "    [matsnu.generate_domain() for i in range(10000)],\n",
    "    columns = ['domain']\n",
    ")\n",
    "matsnu_df['label'] = 'dga'\n",
    "\n",
    "data_df = pd.concat([alexa_df, matsnu_df]).reset_index(drop=True)\n",
    "\n",
    "class_labeler = LabelEncoder()\n",
    "\n",
    "data_df['label'] = class_labeler.fit_transform(data_df['label'])\n",
    "\n",
    "data_df.to_csv(os.path.join(data_dir, \"dga.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/alexa_top_1m.csv' does not exist: b'data/alexa_top_1m.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4b8f6c1b419a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m alexa_df = pd.read_csv(\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alexa_top_1m.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ).iloc[:200]\n\u001b[1;32m      6\u001b[0m \u001b[0malexa_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'benign'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/alexa_top_1m.csv' does not exist: b'data/alexa_top_1m.csv'"
     ]
    }
   ],
   "source": [
    "# Testing dataframe to csv\n",
    "alexa_df = pd.read_csv(\n",
    "    os.path.join(data_dir, 'alexa_top_1m.csv'),\n",
    "    names=['domain']\n",
    ").iloc[:200]\n",
    "alexa_df['label'] = 'benign'\n",
    "\n",
    "matsnu_df = pd.DataFrame(\n",
    "    [matsnu.generate_domain() for i in range(200)],\n",
    "    columns = ['domain']\n",
    ")\n",
    "matsnu_df['label'] = 'dga'\n",
    "\n",
    "data_df = pd.concat([alexa_df, matsnu_df]).reset_index(drop=True)\n",
    "\n",
    "class_labeler = LabelEncoder()\n",
    "\n",
    "data_df['label'] = class_labeler.fit_transform(data_df['label'])\n",
    "\n",
    "data_df.to_csv(os.path.join(data_dir, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/alexa_top_1m.csv' does not exist: b'data/alexa_top_1m.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d733acd7651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m alexa_df = pd.read_csv(\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alexa_top_1m.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'domain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m ).iloc[:200]\n\u001b[1;32m      6\u001b[0m \u001b[0malexa_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'benign'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/alexa_top_1m.csv' does not exist: b'data/alexa_top_1m.csv'"
     ]
    }
   ],
   "source": [
    "# Validation dataframe to csv\n",
    "alexa_df = pd.read_csv(\n",
    "    os.path.join(data_dir, 'alexa_top_1m.csv'),\n",
    "    names=['domain']\n",
    ").iloc[:200]\n",
    "alexa_df['label'] = 'benign'\n",
    "\n",
    "matsnu_df = pd.DataFrame(\n",
    "    [matsnu.generate_domain() for i in range(200)],\n",
    "    columns = ['domain']\n",
    ")\n",
    "matsnu_df['label'] = 'dga'\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_df = pd.concat([alexa_df, matsnu_df]).reset_index(drop=True)\n",
    "\n",
    "data_df = shuffle(data_df).reset_index(drop=True)\n",
    "\n",
    "data_df.head()\n",
    "\n",
    "# class_labeler = LabelEncoder()\n",
    "\n",
    "# data_df['label'] = class_labeler.fit_transform(data_df['label'])\n",
    "\n",
    "# data_df.to_csv(os.path.join(data_dir, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy, x_lens, y_lens\n",
    "\n",
    "def _get_train_test_data_loader(batch_size, data_dir, filename):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(data_dir, filename)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    \n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    \n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "    return train_dl, test_dl\n",
    "\n",
    "def pad_collate_pred(batch):\n",
    "\n",
    "    x_lens = [len(x) for x in batch]\n",
    "\n",
    "    xx_pad = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "def _get_predict_loader(batch_size, data_dir, filename):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(data_dir, filename, train=False)\n",
    "    \n",
    "    predict_dl = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_pred)\n",
    "    \n",
    "    return predict_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EEE\n",
      "======================================================================\n",
      "ERROR: test_classes (__main__.TestDomainDataSet)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-309e05f41c15>\", line 5, in setUp\n",
      "    self.test_dga = DomainDataset('data', 'dga.csv')\n",
      "NameError: name 'DomainDataset' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_getitem (__main__.TestDomainDataSet)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-309e05f41c15>\", line 5, in setUp\n",
      "    self.test_dga = DomainDataset('data', 'dga.csv')\n",
      "NameError: name 'DomainDataset' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_index (__main__.TestDomainDataSet)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-309e05f41c15>\", line 5, in setUp\n",
      "    self.test_dga = DomainDataset('data', 'dga.csv')\n",
      "NameError: name 'DomainDataset' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.006s\n",
      "\n",
      "FAILED (errors=3)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestDomainDataSet(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.test_dga = DomainDataset('data', 'dga.csv')\n",
    "        \n",
    "    def test_classes(self):\n",
    "        expected_classes = [0, 1]\n",
    "        classes = self.test_dga.data_df['label'].unique().tolist()\n",
    "        self.assertEqual(classes, \n",
    "                         expected_classes, \n",
    "                         \"Incorrect classes ({}), expected {})\".format(classes, expected_classes)\n",
    "                        )\n",
    "        \n",
    "    def test_index(self):\n",
    "        index = self.test_dga.data_df.index.tolist()\n",
    "        self.assertEqual(len(index), \n",
    "                         len(set(index)),\n",
    "                         \"Index contains duplicates ({}), expected {})\".format(len(set(index)), len(index)))\n",
    "\n",
    "    def test_getitem(self):\n",
    "        item = self.test_dga.__getitem__(0)\n",
    "        self.assertEqual(len(item), \n",
    "                         2,\n",
    "                         \"Get item output is wrong length ({}), expected {})\".format(len(item), 2))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "class DGAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Estimator for generating sequences of target variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features=65, hidden_dim=12, n_layers=2, output_dim=1, embedding_dim=5, batch_size=10):\n",
    "        super(DGAClassifier, self).__init__()\n",
    "\n",
    "        # Variables\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(input_features, embedding_dim)\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=0.3, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def init_hidden(self):\n",
    "#         \"\"\"\n",
    "#         Initialize the hidden and cell states of the LSTM with zeros.\n",
    "#         \"\"\"\n",
    "#         return torch.zeros(self.hidden_layers, self.batch_size, self.hidden_dim)\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on batch of tracks.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: (batch_size, longest_sequence, embedding) i.e. 10, 32, 5\n",
    "        # hidden size: (hidden_layers, batch_size, hidden_dim) i.e. 2, 10, 30\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # x_embed: (batch_size, longest_sequence, 1?, embedding_size)\n",
    "        embed_x = self.embedding(x)\n",
    "        \n",
    "        x_packed = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        output_packed, hidden_state = self.rnn(x_packed)\n",
    "        \n",
    "        output_padded, lengths = pad_packed_sequence(output_packed, batch_first=False)\n",
    "        \n",
    "        output = output_padded.view(batch_size*seq_len, self.hidden_dim)\n",
    "        \n",
    "        adjusted_lengths = [(l-1)*batch_size + i for i, l in enumerate(lengths)]\n",
    "        \n",
    "        lengthTensor = torch.tensor(adjusted_lengths, dtype=torch.int64)\n",
    "        \n",
    "        output = output.index_select(0, lengthTensor)\n",
    "        \n",
    "        output = output.view(batch_size, self.hidden_dim)\n",
    "        \n",
    "        output = self.sigmoid(self.fc(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed, cuda=False):\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# TO:DO Figure out splunk query\n",
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'options': {'params': {'mode': 'stage', 'algo': 'rnn_dga_classifier', 'epochs': '10', 'batch_size': '1'}, 'args': ['domain', 'label'], 'feature_variables': ['domain', 'label'], 'model_name': 'DGA_App', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '1000', 'max_model_size_mb': '15', 'max_score_time': '600', 'streaming_apply': 'false', 'use_sampling': 'true'}, 'kfold_cv': None}, 'feature_variables': ['domain', 'label']}\n",
      "<bound method NDFrame.describe of                                   domain  label\n",
      "0       dustindicatefaultpressgarden.com      1\n",
      "1          parkschedulebuildpencatch.com      1\n",
      "2         newsattemptexperiencedtake.com      1\n",
      "3           wingconditionpasslecture.com      1\n",
      "4      fooddishwitnessboxtankoperate.com      1\n",
      "...                                  ...    ...\n",
      "19995                      wikipedia.org      0\n",
      "19996                          baidu.com      0\n",
      "19997                       facebook.com      0\n",
      "19998                        youtube.com      0\n",
      "19999                         google.com      0\n",
      "\n",
      "[20000 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"DGA_App\")\n",
    "print(param)\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "filename = 'dga.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "{'feature_variables': ['domain'], 'target_variables': ['benign', 'dga']}\n"
     ]
    }
   ],
   "source": [
    "# # THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "train_dl, test_dl = _get_train_test_data_loader(batch_size, data_dir, filename) # print(df.describe())\n",
    "param = {'feature_variables': ['domain'], 'target_variables': ['benign', 'dga']}\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(param):\n",
    "\n",
    "    # Define params for model\n",
    "#     input_size = 67\n",
    "#     hidden_dim = 30\n",
    "#     num_classes = 1\n",
    "#     learning_rate = 0.001\n",
    "#     embedding_dim = 5\n",
    "#     n_layers = 2\n",
    "    mapping = {0: 'benign', 1: 'dga'}\n",
    "    \n",
    "    model = {\n",
    "        \"input_size\": 67,\n",
    "        \"hidden_dim\": 30,\n",
    "        \"embedding_dim\": 5,\n",
    "        \"num_classes\": 1,\n",
    "        \"n_layers\": 2,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"mapping\": mapping,\n",
    "        \"num_epochs\": 2,\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "    \n",
    "    model[\"train_dl\"], model[\"test_dl\"] = _get_train_test_data_loader(batch_size, data_dir, filename)\n",
    "\n",
    "    print(\"FIT build RNN model with input size \" + str(model[\"train_dl\"].dataset.__len__()))\n",
    "#     print(\"FIT build model with target classes \" + str(model[\"train_dl\"].dataset.labels))\n",
    "    \n",
    "#     if 'options' in param:\n",
    "#         if 'params' in param['options']:\n",
    "#             if 'epochs' in param['options']['params']:\n",
    "#                 model['num_epochs'] = int(param['options']['params']['epochs'])\n",
    "#             if 'batch_size' in param['options']['params']:\n",
    "#                 model['batch_size'] = int(param['options']['params']['batch_size'])\n",
    "\n",
    "    # Initialize DGA Classifier\n",
    "    model['model'] = DGAClassifier(\n",
    "        input_features=model[\"input_size\"], \n",
    "        hidden_dim=model[\"hidden_dim\"], \n",
    "        n_layers=model[\"n_layers\"], \n",
    "        output_dim=model[\"num_classes\"],\n",
    "        embedding_dim=model[\"embedding_dim\"], \n",
    "        batch_size=model[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    model['criterion'] = torch.nn.BCELoss()\n",
    "    \n",
    "    model['optimizer'] = torch.optim.Adam(model['model'].parameters())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "FIT build RNN model with input size 16000\n",
      "{'input_size': 67, 'hidden_dim': 30, 'embedding_dim': 5, 'num_classes': 1, 'n_layers': 2, 'learning_rate': 0.001, 'mapping': {0: 'benign', 1: 'dga'}, 'num_epochs': 2, 'batch_size': 32, 'train_dl': <torch.utils.data.dataloader.DataLoader object at 0x7f0818e51470>, 'test_dl': <torch.utils.data.dataloader.DataLoader object at 0x7f081a8e3198>, 'model': DGAClassifier(\n",
      "  (embedding): Embedding(67, 5)\n",
      "  (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), 'criterion': BCELoss(), 'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_target(label, label_to_ix):\n",
    "#     return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "# label_to_ix = {\"benign\": 0, \"dga\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_categories = [\"benign\", \"dga\"]\n",
    "# def categoryFromOutput(output):\n",
    "#     top_n, top_i = output.topk(1)\n",
    "#     category_i = top_i[0].item()\n",
    "#     return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_acc(y_pred, y_test):\n",
    "#     y_pred_tag = torch.round(y_pred)\n",
    "\n",
    "#     correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "#     acc = correct_results_sum/y_test.shape[0]\n",
    "#     acc = torch.round(acc * 100)\n",
    "    \n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stats(accuracy, confusion_matrix, output, y):\n",
    "    output = torch.round(output).flatten()\n",
    "    equal = torch.eq(output, y)\n",
    "    correct = int(torch.sum(equal))\n",
    "    for j, i in zip(output, y):\n",
    "        confusion_matrix[int(i),int(j)]+=1\n",
    "\n",
    "    return accuracy + correct, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model, param):\n",
    "    \n",
    "    returns = {\"message\": \"model trained\"}\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cpu\") if not cuda else torch.device(\"cuda:0\")\n",
    "    seed_everything(seed=1337, cuda=cuda)\n",
    "    \n",
    "    accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "    \n",
    "    for epoch in range(1, model['num_epochs'] + 1):\n",
    "\n",
    "        # Train\n",
    "        model['model'].train()\n",
    "        print(\"Train ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        t = time.time()\n",
    "\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over dataset\n",
    "        for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"train_dl\"]):\n",
    "            \n",
    "            # Clear stored gradient\n",
    "            model[\"optimizer\"].zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            loss = model['criterion'](output, torch.Tensor(y_padded).unsqueeze(1))\n",
    "\n",
    "            total_loss += float(loss)\n",
    "            \n",
    "            accuracy, confusion_matrix = update_stats(\n",
    "                accuracy, \n",
    "                confusion_matrix, \n",
    "                torch.Tensor(output), \n",
    "                torch.Tensor(y_padded)\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            model[\"optimizer\"].step()\n",
    "                            \n",
    "            print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"train_dl\"]), time.time() - t), end='\\r', flush=True)\n",
    "            t = time.time()\n",
    "            \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"train_dl\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"train_dl\"].dataset), accuracy / len(model[\"train_dl\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        model['model'].eval()\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        t = time.time()\n",
    "        total_loss = 0\n",
    "        print(\"Validation ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"test_dl\"]):\n",
    "                output =  model['model'](x_padded, x_lens)\n",
    "                total_loss += float(model['criterion'](output, torch.Tensor(y_padded).unsqueeze(1)))\n",
    "                accuracy, confusion_matrix = update_stats(\n",
    "                    accuracy, \n",
    "                    confusion_matrix, \n",
    "                    torch.Tensor(output), \n",
    "                    torch.Tensor(y_padded)\n",
    "                )\n",
    "                print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"test_dl\"]), time.time() - t), end='\\r', flush=True)\n",
    "                t = time.time()\n",
    "                \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"test_dl\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"test_dl\"].dataset), accuracy / len(model[\"test_dl\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model['num_epochs']\n",
    "    returns['model_batch_size'] = model['batch_size']\n",
    "    returns['model_loss_acc'] = total_loss / len(model[\"train_dl\"])\n",
    "            \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1)\n",
      "--------------------\n",
      "[Batch]: 499/500 in 0.02580 seconds\n",
      "[Loss]: 0.31319\n",
      "[Accuracy]: 13687/16000 : 85.544%\n",
      "[[6228 1740]\n",
      " [ 573 7459]] \n",
      "\n",
      "Validation (1)\n",
      "--------------------\n",
      "[Batch]: 124/125 in 0.00745 seconds\n",
      "[Loss]: 0.02990\n",
      "[Accuracy]: 3969/4000 : 99.225%\n",
      "[[2001   31]\n",
      " [   0 1968]] \n",
      "\n",
      "Train (2)\n",
      "--------------------\n",
      "[Batch]: 499/500 in 0.02480 seconds\n",
      "[Loss]: 0.01921\n",
      "[Accuracy]: 15918/16000 : 99.487%\n",
      "[[7907   61]\n",
      " [  21 8011]] \n",
      "\n",
      "Validation (2)\n",
      "--------------------\n",
      "[Batch]: 124/125 in 0.00755 seconds\n",
      "[Loss]: 0.01032\n",
      "[Accuracy]: 3990/4000 : 99.750%\n",
      "[[2022   10]\n",
      " [   0 1968]] \n",
      "\n",
      "{'message': 'model trained', 'model_epochs': 2, 'model_batch_size': 32, 'model_loss_acc': 0.002580300260800868}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('benign', 'dga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # print images\n",
    "# # imshow(torchvision.utils.make_grid(images))\n",
    "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = RNNEstimator()\n",
    "# net.load_state_dict(rnn, strict=False)\n",
    "# net.eval()\n",
    "\n",
    "# dataiter = iter(testLoader)\n",
    "# domains, labels, xx_lens, _ = dataiter.next()\n",
    "\n",
    "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(len(labels))))\n",
    "\n",
    "# def evaluate(domains):\n",
    "#     with torch.no_grad():\n",
    "#         for data in testloader:\n",
    "#             input = domains\n",
    "#             hidden = model['model'].init_hidden()\n",
    "#             output = model['model'](input, xx_lens, hidden)[0]\n",
    "#             y_hat = output[:, -1, :]\n",
    "#             print(y_hat)\n",
    "#             _, predicted = torch.max(y_hat, 1)\n",
    "#             prediction = [classes[key] for key in predicted.numpy()]\n",
    "        \n",
    "# evaluate(domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model, filename, param):\n",
    "    predict_dl = _get_predict_loader(batch_size, data_dir, filename)    \n",
    "    classes = model['mapping']\n",
    "    model['model'].eval()    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (x_padded,  x_lens) in enumerate(predict_dl):\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            y_hat = torch.round(output.data)\n",
    "            predictions += [classes[int(key)] for key in y_hat.flatten().numpy()]\n",
    "           \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "['benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'benign', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga']\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,'test.csv',param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apply' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e51b92ff8acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'apply' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = apply(model,'test.csv',param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",
    "    if model is not None:\n",
    "        if 'model' in model:\n",
    "            returns[\"summary\"] = str(model)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
