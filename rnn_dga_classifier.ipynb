{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN DGA Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "from io import open\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\"\n",
    "\n",
    "def seed_everything(seed, cuda=False):\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 752.0MB 53kB/s  eta 0:00:01| 110.0MB 31.3MB/s eta 0:00:2114       | 243.3MB 21.3MB/s eta 0:00:24ta 0:00:28MB 59.8MB/s eta 0:00:06|█████████████████▊              | 416.6MB 30.8MB/s eta 0:00:11B 16.3MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.15.4)\n",
      "Collecting future (from torch)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 7.2MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built future\n",
      "Installing collected packages: future, torch\n",
      "  Found existing installation: torch 1.0.1.post2\n",
      "    Uninstalling torch-1.0.1.post2:\n",
      "      Successfully uninstalled torch-1.0.1.post2\n",
      "Successfully installed future-0.18.2 torch-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 80\n",
      "drwxr-xr-x. 2 root root    70 May 21 23:06 .\n",
      "drwxr-xr-x. 6 root root  4096 May 27 22:26 ..\n",
      "-rw-r--r--. 1 root root  6148 Jul 26  2019 .DS_Store\n",
      "-rw-r--r--. 1 root root 33095 May 21 22:55 DGA_App.pt\n",
      "-rw-r--r--. 1 root root 15004 May 21 23:15 dga.pt\n",
      "-rw-r--r--. 1 root root 15004 May 27 22:34 dga.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -la \"/srv/app/model/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.15.4\n",
      "pandas version: 0.25.1\n",
      "PyTorch: 1.0.1.post2\n",
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"PyTorch: \" + torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer mode=stage algo=barebone epochs=10 batch_size=1 s from feature_* into app:barebone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# TO:DO Figure out splunk query\n",
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  domain  label\n",
      "0       dustindicatefaultpressgarden.com      1\n",
      "1          parkschedulebuildpencatch.com      1\n",
      "2         newsattemptexperiencedtake.com      1\n",
      "3           wingconditionpasslecture.com      1\n",
      "4      fooddishwitnessboxtankoperate.com      1\n",
      "...                                  ...    ...\n",
      "19995                      wikipedia.org      0\n",
      "19996                          baidu.com      0\n",
      "19997                       facebook.com      0\n",
      "19998                        youtube.com      0\n",
      "19999                         google.com      0\n",
      "\n",
      "[20000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"dga\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): directory name\n",
    "            csv_filename (string): csv filename\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_df = df\n",
    "        \n",
    "        self.all_chars =  self.__build__chars__()\n",
    "        self.inputs = self.data_df.iloc[:, 0]\n",
    "                                   \n",
    "        self.train = train\n",
    "                                   \n",
    "        if self.train:\n",
    "            self.labels = self.data_df.iloc[:, -1]\n",
    "        \n",
    "        self.data_len = len(self.data_df.index)\n",
    "\n",
    "    def __build__chars__(self):\n",
    "        \"\"\"Build dictionary of chars.\"\"\"\n",
    "        all_letters = string.ascii_letters + string.digits + \" .'-\"\n",
    "        return {all_letters[i]:i+1 for i in range(0, len(all_letters))}\n",
    "    \n",
    "    def char_to_ix(self, char):\n",
    "        \"\"\"Character to index lookup.\"\"\"\n",
    "        return self.all_chars[char]\n",
    "\n",
    "    def ix_to_char(self, char):\n",
    "        \"\"\"Index to character lookup.\"\"\"\n",
    "        for i, val in self.all_chars.items():\n",
    "            if val == char:\n",
    "                return i\n",
    "\n",
    "    def domain_to_ix(self, domain):\n",
    "        \"\"\"Domain to sequence of indexes.\"\"\"\n",
    "        return torch.LongTensor([self.char_to_ix(i) for i in domain])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        domain = self.domain_to_ix(self.inputs[index])\n",
    "        if self.train:\n",
    "            target = torch.Tensor([self.labels[index]])\n",
    "            return domain, target\n",
    "        else:\n",
    "            return domain\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy, x_lens, y_lens\n",
    "\n",
    "def _get_train_test_data_loader(batch_size, df):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(df, train=True)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    \n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    \n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "    return train_dl, test_dl\n",
    "\n",
    "def pad_collate_pred(batch):\n",
    "\n",
    "    x_lens = [len(x) for x in batch]\n",
    "\n",
    "    xx_pad = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "def _get_predict_loader(batch_size, df):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(df, train=False)\n",
    "    \n",
    "    predict_dl = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_pred)\n",
    "    \n",
    "    return predict_dl\n",
    "\n",
    "\n",
    "class DGAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Estimator for generating sequences of target variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features=65, hidden_dim=12, n_layers=2, output_dim=1, embedding_dim=5, batch_size=10):\n",
    "        super(DGAClassifier, self).__init__()\n",
    "\n",
    "        # Variables\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(input_features, embedding_dim)\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=0.3, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on batch of tracks.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: (batch_size, longest_sequence, embedding) i.e. 10, 32, 5\n",
    "        # hidden size: (hidden_layers, batch_size, hidden_dim) i.e. 2, 10, 30\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # x_embed: (batch_size, longest_sequence, 1?, embedding_size)\n",
    "        embed_x = self.embedding(x)\n",
    "        \n",
    "        x_packed = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        output_packed, hidden_state = self.rnn(x_packed)\n",
    "        \n",
    "        output_padded, lengths = pad_packed_sequence(output_packed, batch_first=False)\n",
    "        \n",
    "        output = output_padded.view(batch_size*seq_len, self.hidden_dim)\n",
    "        \n",
    "        adjusted_lengths = [(l-1)*batch_size + i for i, l in enumerate(lengths)]\n",
    "        \n",
    "        lengthTensor = torch.tensor(adjusted_lengths, dtype=torch.int64)\n",
    "        \n",
    "        output = output.index_select(0, lengthTensor)\n",
    "        \n",
    "        output = output.view(batch_size, self.hidden_dim)\n",
    "        \n",
    "        output = self.sigmoid(self.fc(output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "def init(df, param):\n",
    "\n",
    "    mapping = {0: 'benign', 1: 'dga'}\n",
    "    \n",
    "    model = {\n",
    "        \"input_size\": 67,\n",
    "        \"hidden_dim\": 30,\n",
    "        \"embedding_dim\": 5,\n",
    "        \"num_classes\": 1,\n",
    "        \"n_layers\": 2,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"mapping\": mapping,\n",
    "        \"num_epochs\": 2,\n",
    "        \"batch_size\": 32,\n",
    "    }\n",
    "    \n",
    "    model[\"train_dl\"], model[\"test_dl\"] = _get_train_test_data_loader(int(model['batch_size']), df)\n",
    "\n",
    "    print(\"FIT build RNN model with input size \" + str(model[\"train_dl\"].dataset.__len__()))\n",
    "\n",
    "    # Initialize DGA Classifier\n",
    "    model['model'] = DGAClassifier(\n",
    "        input_features=model[\"input_size\"], \n",
    "        hidden_dim=model[\"hidden_dim\"], \n",
    "        n_layers=model[\"n_layers\"], \n",
    "        output_dim=model[\"num_classes\"],\n",
    "        embedding_dim=model[\"embedding_dim\"], \n",
    "        batch_size=model[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    model['criterion'] = torch.nn.BCELoss()\n",
    "    \n",
    "    model['optimizer'] = torch.optim.Adam(model['model'].parameters())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "FIT build RNN model with input size 16000\n",
      "{'input_size': 67, 'hidden_dim': 30, 'embedding_dim': 5, 'num_classes': 1, 'n_layers': 2, 'learning_rate': 0.001, 'mapping': {0: 'benign', 1: 'dga'}, 'num_epochs': 2, 'batch_size': 32, 'train_dl': <torch.utils.data.dataloader.DataLoader object at 0x7f15b5bfbba8>, 'test_dl': <torch.utils.data.dataloader.DataLoader object at 0x7f15b5bfbc18>, 'model': DGAClassifier(\n",
      "  (embedding): Embedding(67, 5)\n",
      "  (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), 'criterion': BCELoss(), 'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df, param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def update_stats(accuracy, confusion_matrix, output, y):\n",
    "    output = torch.round(output).flatten()\n",
    "    equal = torch.eq(output, y)\n",
    "    correct = int(torch.sum(equal))\n",
    "    for j, i in zip(output, y):\n",
    "        confusion_matrix[int(i),int(j)]+=1\n",
    "\n",
    "    return accuracy + correct, confusion_matrix\n",
    "\n",
    "def fit(model, df, param):\n",
    "    \n",
    "    returns = {\"message\": \"model trained\"}\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cpu\") if not cuda else torch.device(\"cuda:0\")\n",
    "    seed_everything(seed=1337, cuda=cuda)\n",
    "    \n",
    "    accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "    \n",
    "    for epoch in range(1, model['num_epochs'] + 1):\n",
    "\n",
    "        # Train\n",
    "        model['model'].train()\n",
    "        print(\"Train ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        t = time.time()\n",
    "\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over dataset\n",
    "        for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"train_dl\"]):\n",
    "            \n",
    "            # Clear stored gradient\n",
    "            model[\"optimizer\"].zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            loss = model['criterion'](output, torch.Tensor(y_padded).unsqueeze(1))\n",
    "\n",
    "            total_loss += float(loss)\n",
    "            \n",
    "            accuracy, confusion_matrix = update_stats(\n",
    "                accuracy, \n",
    "                confusion_matrix, \n",
    "                torch.Tensor(output), \n",
    "                torch.Tensor(y_padded)\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            model[\"optimizer\"].step()\n",
    "            if batch_num % 100 == 0:\n",
    "                print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"train_dl\"]), time.time() - t), end='\\r', flush=True)\n",
    "            t = time.time()\n",
    "            \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"train_dl\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"train_dl\"].dataset), accuracy / len(model[\"train_dl\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        model['model'].eval()\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        t = time.time()\n",
    "        total_loss = 0\n",
    "        print(\"Validation ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"test_dl\"]):\n",
    "                output =  model['model'](x_padded, x_lens)\n",
    "                total_loss += float(model['criterion'](output, torch.Tensor(y_padded).unsqueeze(1)))\n",
    "                accuracy, confusion_matrix = update_stats(\n",
    "                    accuracy, \n",
    "                    confusion_matrix, \n",
    "                    torch.Tensor(output), \n",
    "                    torch.Tensor(y_padded)\n",
    "                )\n",
    "                if batch_num % 50 == 0:\n",
    "                    print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"test_dl\"]), time.time() - t), end='\\r', flush=True)\n",
    "                t = time.time()\n",
    "                \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"test_dl\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"test_dl\"].dataset), accuracy / len(model[\"test_dl\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model['num_epochs']\n",
    "    returns['model_batch_size'] = model['batch_size']\n",
    "    returns['model_loss_acc'] = total_loss / len(model[\"train_dl\"])\n",
    "            \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1)\n",
      "--------------------\n",
      "[Batch]: 400/500 in 0.02491 seconds\n",
      "[Loss]: 0.01541\n",
      "[Accuracy]: 15940/16000 : 99.625%\n",
      "[[7938   48]\n",
      " [  12 8002]] \n",
      "\n",
      "Validation (1)\n",
      "--------------------\n",
      "[Batch]: 100/125 in 0.00725 seconds\n",
      "[Loss]: 0.01332\n",
      "[Accuracy]: 3989/4000 : 99.725%\n",
      "[[2003   11]\n",
      " [   0 1986]] \n",
      "\n",
      "Train (2)\n",
      "--------------------\n",
      "[Batch]: 400/500 in 0.02603 seconds\n",
      "[Loss]: 0.01240\n",
      "[Accuracy]: 15952/16000 : 99.700%\n",
      "[[7945   41]\n",
      " [   7 8007]] \n",
      "\n",
      "Validation (2)\n",
      "--------------------\n",
      "[Batch]: 100/125 in 0.01019 seconds\n",
      "[Loss]: 0.01338\n",
      "[Accuracy]: 3988/4000 : 99.700%\n",
      "[[2002   12]\n",
      " [   0 1986]] \n",
      "\n",
      "{'message': 'model trained', 'model_epochs': 2, 'model_batch_size': 32, 'model_loss_acc': 0.0033458294686861336}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,df,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model, df, param):\n",
    "    predict_dl = _get_predict_loader(32, df)    \n",
    "    classes = mapping = {0: 'benign', 1: 'dga'}\n",
    "    model['model'].eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (x_padded,  x_lens) in enumerate(predict_dl):\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            y_hat = torch.round(output.data)\n",
    "            predictions += [classes[int(key)] for key in y_hat.flatten().numpy()]\n",
    "           \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "['dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga']\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(test,df,param)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model, name):\n",
    "    torch.save(model['model'].state_dict(), MODEL_DIRECTORY + \"dga\" + \".pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 80\n",
      "drwxr-xr-x. 2 root root    70 May 21 23:06 .\n",
      "drwxr-xr-x. 6 root root  4096 May 27 22:26 ..\n",
      "-rw-r--r--. 1 root root  6148 Jul 26  2019 .DS_Store\n",
      "-rw-r--r--. 1 root root 33095 May 21 22:55 DGA_App.pt\n",
      "-rw-r--r--. 1 root root 15004 May 21 23:15 dga.pt\n",
      "-rw-r--r--. 1 root root 15004 May 27 22:31 dga.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -la /srv/app/model/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 67,\n",
       " 'hidden_dim': 30,\n",
       " 'embedding_dim': 5,\n",
       " 'num_classes': 1,\n",
       " 'n_layers': 2,\n",
       " 'learning_rate': 0.001,\n",
       " 'mapping': {0: 'benign', 1: 'dga'},\n",
       " 'num_epochs': 2,\n",
       " 'batch_size': 32,\n",
       " 'train_dl': <torch.utils.data.dataloader.DataLoader at 0x7f15b5bfbba8>,\n",
       " 'test_dl': <torch.utils.data.dataloader.DataLoader at 0x7f15b5bfbc18>,\n",
       " 'model': DGAClassifier(\n",
       "   (embedding): Embedding(67, 5)\n",
       "   (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\n",
       "   (fc): Linear(in_features=30, out_features=1, bias=True)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " 'criterion': BCELoss(),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.001\n",
       "     weight_decay: 0\n",
       " )}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(model, \"dga\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    model = {}\n",
    "    dga_model = DGAClassifier(67, 30, 2, 1, 5, 32)\n",
    "    dga_model.load_state_dict(torch.load(MODEL_DIRECTORY + \"dga\" + \".pth\"))\n",
    "    model['model'] = dga_model\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load('dga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "['dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga', 'dga']\n"
     ]
    }
   ],
   "source": [
    "print(apply(test,df,param)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",
    "    if model is not None:\n",
    "        if 'model' in model:\n",
    "            returns[\"summary\"] = str(model)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': {'pytorch': '1.5.0'},\n",
       " 'summary': \"{'input_size': 67, 'hidden_dim': 30, 'embedding_dim': 5, 'num_classes': 1, 'n_layers': 2, 'learning_rate': 0.001, 'mapping': {0: 'benign', 1: 'dga'}, 'num_epochs': 2, 'batch_size': 32, 'train_dl': <torch.utils.data.dataloader.DataLoader object at 0x7fb7e6014a20>, 'test_dl': <torch.utils.data.dataloader.DataLoader object at 0x7fb7e6014e48>, 'model': DGAClassifier(\\n  (embedding): Embedding(67, 5)\\n  (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\\n  (fc): Linear(in_features=30, out_features=1, bias=True)\\n  (sigmoid): Sigmoid()\\n), 'criterion': BCELoss(), 'optimizer': Adam (\\nParameter Group 0\\n    amsgrad: False\\n    betas: (0.9, 0.999)\\n    eps: 1e-08\\n    lr: 0.001\\n    weight_decay: 0\\n)}\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
