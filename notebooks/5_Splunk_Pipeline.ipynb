{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGA Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "\n",
    "import json\n",
    "from io import open\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from dga.tools import matsnu\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dga.models.dga_classifier import DGAClassifier\n",
    "from dga.datasets.domain_dataset import DomainDataset\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "# global constants\n",
    "# MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"PyTorch: \" + torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",
    "    for i in range(0,torch.cuda.device_count()):\n",
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get sample data\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DomainDataset(Dataset):\n",
    "#     def __init__(self, df, train=True):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             data_dir (string): directory name\n",
    "#             csv_filename (string): csv filename\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.data_df = df\n",
    "        \n",
    "#         self.all_chars =  self.__build__chars__()\n",
    "#         self.inputs = self.data_df.iloc[:, 0]\n",
    "                                   \n",
    "#         self.train = train\n",
    "                                   \n",
    "#         if self.train:\n",
    "#             self.labels = self.data_df.iloc[:, -1]\n",
    "        \n",
    "#         self.data_len = len(self.data_df.index)\n",
    "\n",
    "#     def __build__chars__(self):\n",
    "#         \"\"\"Build dictionary of chars.\"\"\"\n",
    "#         all_letters = string.ascii_letters + string.digits + \" .'-\"\n",
    "#         return {all_letters[i]:i+1 for i in range(0, len(all_letters))}\n",
    "    \n",
    "#     def char_to_ix(self, char):\n",
    "#         \"\"\"Character to index lookup.\"\"\"\n",
    "#         return self.all_chars[char]\n",
    "\n",
    "#     def ix_to_char(self, char):\n",
    "#         \"\"\"Index to character lookup.\"\"\"\n",
    "#         for i, val in self.all_chars.items():\n",
    "#             if val == char:\n",
    "#                 return i\n",
    "\n",
    "#     def domain_to_ix(self, domain):\n",
    "#         \"\"\"Domain to sequence of indexes.\"\"\"\n",
    "#         return torch.LongTensor([self.char_to_ix(i) for i in domain])\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         domain = self.domain_to_ix(self.inputs[index])\n",
    "#         if self.train:\n",
    "#             target = torch.Tensor([self.labels[index]])\n",
    "#             return domain, target\n",
    "#         return domain\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy, x_lens, y_lens\n",
    "\n",
    "def _get_train_test_data_loader(batch_size, df, test_split):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(df, train=True)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    \n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    \n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "    return train_dl, test_dl\n",
    "\n",
    "def pad_collate_pred(batch):\n",
    "\n",
    "    x_lens = [len(x) for x in batch]\n",
    "\n",
    "    xx_pad = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "def _get_predict_loader(batch_size, df):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(df, train=False)\n",
    "    \n",
    "    predict_dl = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_pred)\n",
    "    \n",
    "    return predict_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "# class DGAClassifier(nn.Module):\n",
    "#     \"\"\"\n",
    "#     RNN Estimator for generating sequences of target variables.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, input_features=65, hidden_dim=12, n_layers=2, output_dim=1, embedding_dim=5, batch_size=10):\n",
    "#         super(DGAClassifier, self).__init__()\n",
    "\n",
    "#         # Variables\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.hidden_layers = n_layers\n",
    "#         self.batch_size = batch_size\n",
    "#         self.embedding_dim = embedding_dim\n",
    "        \n",
    "#         # Embedding\n",
    "#         self.embedding = nn.Embedding(input_features, embedding_dim)\n",
    "        \n",
    "#         # RNN Layer\n",
    "#         self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=0.3, batch_first=True)\n",
    "        \n",
    "#         # Fully connected layer\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x, x_lens):\n",
    "#         \"\"\"\n",
    "#         Perform a forward pass of our model on batch of tracks.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # x: (batch_size, longest_sequence, embedding) i.e. 10, 32, 5\n",
    "#         # hidden size: (hidden_layers, batch_size, hidden_dim) i.e. 2, 10, 30\n",
    "#         batch_size, seq_len = x.size()\n",
    "\n",
    "#         # x_embed: (batch_size, longest_sequence, 1?, embedding_size)\n",
    "#         embed_x = self.embedding(x)\n",
    "        \n",
    "#         x_packed = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "#         # Passing in the input and hidden state into the model and obtaining outputs\n",
    "#         output_packed, hidden_state = self.rnn(x_packed)\n",
    "        \n",
    "#         output_padded, lengths = pad_packed_sequence(output_packed, batch_first=False)\n",
    "        \n",
    "#         output = output_padded.view(batch_size*seq_len, self.hidden_dim)\n",
    "        \n",
    "#         adjusted_lengths = [(l-1)*batch_size + i for i, l in enumerate(lengths)]\n",
    "        \n",
    "#         lengthTensor = torch.tensor(adjusted_lengths, dtype=torch.int64)\n",
    "        \n",
    "#         output = output.index_select(0, lengthTensor)\n",
    "        \n",
    "#         output = output.view(batch_size, self.hidden_dim)\n",
    "        \n",
    "#         output = self.sigmoid(self.fc(output))\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed, cuda=False):\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# TO:DO Figure out splunk query\n",
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(filename):\n",
    "    # with open(\"../data/\"+ filename, 'r') as f:\n",
    "    df = pd.read_csv(\"../data/\" + filename)\n",
    "    # with open(\"data/\" + name + \".json\", 'r') as f:\n",
    "    #     param = json.load(f)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.describe of                         domain            family  label\n",
      "0                  136658.live          majestic      0\n",
      "1             itwprobrands.com          majestic      0\n",
      "2                 marketpet.ru          majestic      0\n",
      "3                   sentry.com          majestic      0\n",
      "4               levitraoua.com          majestic      0\n",
      "...                        ...               ...    ...\n",
      "999995    ivvmqopgnkofotqj.net   murofet_dga.csv      1\n",
      "999996       wdbrupsclidxw.biz     locky_dga.csv      1\n",
      "999997   montgomeryjessamyn.ru  suppobox_dga.csv      1\n",
      "999998   captainunderstood.net  suppobox_dga.csv      1\n",
      "999999  ghzjogvieufyrzblkl.biz    qakbot_dga.csv      1\n",
      "\n",
      "[1000000 rows x 3 columns]>\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df = stage(\"1595390875_sample_dga.csv\")\n",
    "# print(param)\n",
    "print(df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>family</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136658.live</td>\n",
       "      <td>majestic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>itwprobrands.com</td>\n",
       "      <td>majestic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketpet.ru</td>\n",
       "      <td>majestic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentry.com</td>\n",
       "      <td>majestic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>levitraoua.com</td>\n",
       "      <td>majestic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>ivvmqopgnkofotqj.net</td>\n",
       "      <td>murofet_dga.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>wdbrupsclidxw.biz</td>\n",
       "      <td>locky_dga.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>montgomeryjessamyn.ru</td>\n",
       "      <td>suppobox_dga.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>captainunderstood.net</td>\n",
       "      <td>suppobox_dga.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>ghzjogvieufyrzblkl.biz</td>\n",
       "      <td>qakbot_dga.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        domain            family  label\n",
       "0                  136658.live          majestic      0\n",
       "1             itwprobrands.com          majestic      0\n",
       "2                 marketpet.ru          majestic      0\n",
       "3                   sentry.com          majestic      0\n",
       "4               levitraoua.com          majestic      0\n",
       "...                        ...               ...    ...\n",
       "999995    ivvmqopgnkofotqj.net   murofet_dga.csv      1\n",
       "999996       wdbrupsclidxw.biz     locky_dga.csv      1\n",
       "999997   montgomeryjessamyn.ru  suppobox_dga.csv      1\n",
       "999998   captainunderstood.net  suppobox_dga.csv      1\n",
       "999999  ghzjogvieufyrzblkl.biz    qakbot_dga.csv      1\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "# train_dl, test_dl = _get_train_test_data_loader(batch_size, df) # print(df.describe())\n",
    "# param = {'feature_variables': ['domain'], 'target_variables': ['benign', 'dga']}\n",
    "# print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df):\n",
    "\n",
    "    mapping = {0: 'benign', 1: 'dga'}\n",
    "    \n",
    "    model = {\n",
    "        \"input_size\": 67,\n",
    "        \"hidden_dim\": 30,\n",
    "        \"embedding_dim\": 5,\n",
    "        \"num_classes\": 1,\n",
    "        \"n_layers\": 2,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"mapping\": mapping,\n",
    "        \"num_epochs\": 2,\n",
    "        \"batch_size\": 10,\n",
    "    }\n",
    "    \n",
    "    model[\"train_dl\"], model[\"test_dl\"] = _get_train_test_data_loader(model['batch_size'], df)\n",
    "\n",
    "    print(\"FIT build RNN model with input size \" + str(model[\"train_dl\"].dataset.__len__()))\n",
    "\n",
    "    # Initialize DGA Classifier\n",
    "    model['model'] = DGAClassifier(\n",
    "        input_features=model[\"input_size\"], \n",
    "        hidden_dim=model[\"hidden_dim\"], \n",
    "        n_layers=model[\"n_layers\"], \n",
    "        output_dim=model[\"num_classes\"],\n",
    "        embedding_dim=model[\"embedding_dim\"], \n",
    "        batch_size=model[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    model['criterion'] = torch.nn.BCELoss()\n",
    "    \n",
    "    model['optimizer'] = torch.optim.Adam(model['model'].parameters())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "FIT build RNN model with input size 800000\n",
      "{'input_size': 67, 'hidden_dim': 30, 'embedding_dim': 5, 'num_classes': 1, 'n_layers': 2, 'learning_rate': 0.001, 'mapping': {0: 'benign', 1: 'dga'}, 'num_epochs': 2, 'batch_size': 10, 'train_dl': <torch.utils.data.dataloader.DataLoader object at 0x7fa153ef7450>, 'test_dl': <torch.utils.data.dataloader.DataLoader object at 0x7fa153ef7d10>, 'model': DGAClassifier(\n",
      "  (embedding): Embedding(67, 5)\n",
      "  (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), 'criterion': BCELoss(), 'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def update_stats(accuracy, confusion_matrix, output, y):\n",
    "    output = torch.round(output).flatten()\n",
    "    equal = torch.eq(output, y)\n",
    "    correct = int(torch.sum(equal))\n",
    "    for j, i in zip(output, y):\n",
    "        confusion_matrix[int(i),int(j)]+=1\n",
    "\n",
    "    return accuracy + correct, confusion_matrix\n",
    "\n",
    "def fit(model):\n",
    "    \n",
    "    returns = {\"message\": \"model trained\"}\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cpu\") if not cuda else torch.device(\"cuda:0\")\n",
    "    seed_everything(seed=1337, cuda=cuda)\n",
    "    \n",
    "    accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "    \n",
    "    for epoch in range(1, model['num_epochs'] + 1):\n",
    "\n",
    "        # Train\n",
    "        model['model'].train()\n",
    "        print(\"Train ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        t = time.time()\n",
    "\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over dataset\n",
    "        for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"train_dl\"]):\n",
    "            \n",
    "            # Clear stored gradient\n",
    "            model[\"optimizer\"].zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            loss = model['criterion'](output, torch.Tensor(y_padded).unsqueeze(1))\n",
    "\n",
    "            total_loss += float(loss)\n",
    "            \n",
    "            accuracy, confusion_matrix = update_stats(\n",
    "                accuracy, \n",
    "                confusion_matrix, \n",
    "                torch.Tensor(output), \n",
    "                torch.Tensor(y_padded)\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            model[\"optimizer\"].step()\n",
    "                            \n",
    "            print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"train_dl\"]), time.time() - t), end='\\r', flush=True)\n",
    "            t = time.time()\n",
    "            \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"train_dl\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"train_dl\"].dataset), accuracy / len(model[\"train_dl\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        model['model'].eval()\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        t = time.time()\n",
    "        total_loss = 0\n",
    "        print(\"Validation ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"test_dl\"]):\n",
    "                output =  model['model'](x_padded, x_lens)\n",
    "                total_loss += float(model['criterion'](output, torch.Tensor(y_padded).unsqueeze(1)))\n",
    "                accuracy, confusion_matrix = update_stats(\n",
    "                    accuracy, \n",
    "                    confusion_matrix, \n",
    "                    torch.Tensor(output), \n",
    "                    torch.Tensor(y_padded)\n",
    "                )\n",
    "                print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"test_dl\"]), time.time() - t), end='\\r', flush=True)\n",
    "                t = time.time()\n",
    "                \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"test_dl\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"test_dl\"].dataset), accuracy / len(model[\"test_dl\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model['num_epochs']\n",
    "    returns['model_batch_size'] = model['batch_size']\n",
    "    returns['model_loss_acc'] = total_loss / len(model[\"train_dl\"])\n",
    "            \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1)\n",
      "--------------------\n",
      "[Batch]: 79999/80000 in 0.02274 seconds\n",
      "[Loss]: 0.20206\n",
      "[Accuracy]: 733942/800000 : 91.743%\n",
      "[[373125  27217]\n",
      " [ 38841 360817]] \n",
      "\n",
      "Validation (1)\n",
      "--------------------\n",
      "[Batch]: 19999/20000 in 0.00687 seconds\n",
      "[Loss]: 0.16080\n",
      "[Accuracy]: 187155/200000 : 93.578%\n",
      "[[96318  3340]\n",
      " [ 9505 90837]] \n",
      "\n",
      "Train (2)\n",
      "--------------------\n",
      "[Batch]: 79999/80000 in 0.02556 seconds\n",
      "[Loss]: 0.16451\n",
      "[Accuracy]: 747570/800000 : 93.446%\n",
      "[[379468  20874]\n",
      " [ 31556 368102]] \n",
      "\n",
      "Validation (2)\n",
      "--------------------\n",
      "[Batch]: 19999/20000 in 0.00779 seconds\n",
      "[Loss]: 0.15094\n",
      "[Accuracy]: 187800/200000 : 93.900%\n",
      "[[96873  2785]\n",
      " [ 9415 90927]] \n",
      "\n",
      "{'message': 'model trained', 'model_epochs': 2, 'model_batch_size': 10, 'model_loss_acc': 0.03773383826772042}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "# TODO: Create genereic prediction method\n",
    "def apply(model, df, param):\n",
    "    predict_dl = _get_predict_loader(int(param['options']['params']['batch_size']), df)    \n",
    "    classes = model['mapping']\n",
    "    model['model'].eval()    \n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (x_padded,  x_lens) in enumerate(predict_dl):\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            y_hat = torch.round(output.data)\n",
    "            predictions += [classes[int(key)] for key in y_hat.flatten().numpy()]\n",
    "           \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "['dga', 'dga', 'dga', 'dga', 'dga']\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    ts = str(time.time()).split('.')[0]\n",
    "    filepath = f'{MODEL_DIR}{ts}_{name}.pt'\n",
    "    torch.save(model, filepath)\n",
    "    return model"
   ]
  },
  {
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilleal/anaconda3/envs/threat_science/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DGAClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.DGAClassifier'>: it's not the same object as __main__.DGAClassifier",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-77ee527474d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../models/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dga\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-c802661fcc02>\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, name)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{MODEL_DIR}{ts}_{name}.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/threat_science/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/threat_science/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/threat_science/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/threat_science/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.DGAClassifier'>: it's not the same object as __main__.DGAClassifier"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = '../models/'\n",
    "save(model,\"dga\")"
   ]
  },
  {
   "cell_type": "markdown",
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",
    "    if model is not None:\n",
    "        if 'model' in model:\n",
    "            returns[\"summary\"] = str(model)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threat_science",
   "language": "python",
   "name": "threat_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
