{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Toolkit for Splunk - Barebone Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "# ...\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 100\n",
      "drwxr-xr-x. 2 root root    37 May  7 21:30 .\n",
      "drwxr-xr-x. 4 root root  4096 May  7 19:30 ..\n",
      "-rw-r--r--. 1 root root  6148 Jul 26  2019 .DS_Store\n",
      "-rw-r--r--. 1 root root 87096 May  8 00:52 dga.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -la \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.15.4\n",
      "pandas version: 0.25.1\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer mode=stage algo=barebone epochs=10 batch_size=1 s from feature_* into app:barebone_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leathercountermouthcommunicate.com\n",
      "sidecombinejokedisappointed.com\n",
      "advantageadmireborderinvite.com\n",
      "hatabusestrugglereservebook.com\n",
      "futurepostaddressplaypage.com\n",
      "numberadaptpotcommentprint.com\n",
      "creamlookdetermineappropriate.com\n",
      "newsweredetailpopestimate.com\n",
      "winnercoatbitwakespitewarn.com\n",
      "ballclockreasontripbuild.com\n"
     ]
    }
   ],
   "source": [
    "# take a look at some matsnu example domains\n",
    "from dga import matsnu\n",
    "\n",
    "for i in range(10):\n",
    "    print(matsnu.generate_domain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matsnu Shape: (20000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>championshiptextorganize.com</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>considerationbecomesalary.com</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>screenawarddecidecampaign.com</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>newsdigexerciseoilsolvereceive.com</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>spotamazinginvitedevilconcern.com</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               domain label\n",
       "0        championshiptextorganize.com   dga\n",
       "1       considerationbecomesalary.com   dga\n",
       "2       screenawarddecidecampaign.com   dga\n",
       "3  newsdigexerciseoilsolvereceive.com   dga\n",
       "4   spotamazinginvitedevilconcern.com   dga"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matsnu domains\n",
    "matsnu_list = []\n",
    "\n",
    "for i in range(20000):\n",
    "    matsnu_list.append(matsnu.generate_domain())\n",
    "    \n",
    "matsnu_df = pd.DataFrame(matsnu_list, columns=['domain'])\n",
    "\n",
    "print(\"Matsnu Shape:\", matsnu_df.shape)\n",
    "\n",
    "matsnu_df['label'] = 'dga'\n",
    "\n",
    "matsnu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>baidu.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain   label\n",
       "1     google.com  benign\n",
       "2    youtube.com  benign\n",
       "3   facebook.com  benign\n",
       "4      baidu.com  benign\n",
       "5  wikipedia.org  benign"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alex top 1 million domains\n",
    "alexa_df = pd.read_csv(data_dir + \"/alexa_top_1m.csv\", names=['domain'])\n",
    "\n",
    "alexa_df['label'] = 'benign'\n",
    "\n",
    "alexa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([alexa_df.iloc[:20000], matsnu_df.iloc[:20000]], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>google.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>baidu.com</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain   label\n",
       "0     google.com  benign\n",
       "1    youtube.com  benign\n",
       "2   facebook.com  benign\n",
       "3      baidu.com  benign\n",
       "4  wikipedia.org  benign"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# instantiate labelencoder object\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply le on categorical feature columns\n",
    "train_df['label'] = le.fit_transform(train_df['label'])\n",
    "\n",
    "train_df['label'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataframe to csv\n",
    "alexa_df = pd.read_csv(\n",
    "    os.path.join(data_dir, 'alexa_top_1m.csv'),\n",
    "    names=['domain']\n",
    ").iloc[:10000]\n",
    "alexa_df['label'] = 'benign'\n",
    "\n",
    "matsnu_df = pd.DataFrame(\n",
    "    [matsnu.generate_domain() for i in range(10000)],\n",
    "    columns = ['domain']\n",
    ")\n",
    "matsnu_df['label'] = 'dga'\n",
    "\n",
    "data_df = pd.concat([alexa_df, matsnu_df]).reset_index(drop=True)\n",
    "\n",
    "class_labeler = LabelEncoder()\n",
    "\n",
    "data_df['class'] = class_labeler.fit_transform(data_df['label'])\n",
    "\n",
    "data_df.to_csv(os.path.join(data_dir, \"dga.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>google.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>baidu.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19995</td>\n",
       "      <td>sentencezoneinvestigatemeet.com</td>\n",
       "      <td>dga</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19996</td>\n",
       "      <td>pressureriskopensmartwise.com</td>\n",
       "      <td>dga</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19997</td>\n",
       "      <td>stresstrackbitesunthinkalarm.com</td>\n",
       "      <td>dga</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19998</td>\n",
       "      <td>painfarmgradefalladaptempty.com</td>\n",
       "      <td>dga</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>sexproposedchallengespirit.com</td>\n",
       "      <td>dga</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 domain   label  class\n",
       "0                            google.com  benign      0\n",
       "1                           youtube.com  benign      0\n",
       "2                          facebook.com  benign      0\n",
       "3                             baidu.com  benign      0\n",
       "4                         wikipedia.org  benign      0\n",
       "...                                 ...     ...    ...\n",
       "19995   sentencezoneinvestigatemeet.com     dga      1\n",
       "19996     pressureriskopensmartwise.com     dga      1\n",
       "19997  stresstrackbitesunthinkalarm.com     dga      1\n",
       "19998   painfarmgradefalladaptempty.com     dga      1\n",
       "19999    sexproposedchallengespirit.com     dga      1\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "# print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "class DomainDataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_filename, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): directory name\n",
    "            csv_filename (string): csv filename\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_df = pd.read_csv(os.path.join(data_dir, csv_filename))\n",
    "        \n",
    "        self.all_chars =  self.__build__chars__()\n",
    "        self.inputs = self.data_df.iloc[:, 0]\n",
    "                                   \n",
    "        self.train = train\n",
    "                                   \n",
    "        if self.train:\n",
    "            self.labels = self.data_df.iloc[:, -1]\n",
    "        \n",
    "        self.data_len = len(self.data_df.index)\n",
    "\n",
    "    def __build__chars__(self):\n",
    "        \"\"\"Build dictionary of chars.\"\"\"\n",
    "        all_letters = string.ascii_letters + string.digits + \" .'-\"\n",
    "        return {all_letters[i]:i+1 for i in range(0, len(all_letters))}\n",
    "    \n",
    "    def char_to_ix(self, char):\n",
    "        \"\"\"Character to index lookup.\"\"\"\n",
    "        return self.all_chars[char]\n",
    "\n",
    "    def ix_to_char(self, char):\n",
    "        \"\"\"Index to character lookup.\"\"\"\n",
    "        for i, val in self.all_chars.items():\n",
    "            if val == char:\n",
    "                return i\n",
    "\n",
    "    def domain_to_ix(self, domain):\n",
    "        \"\"\"Domain to sequence of indexes.\"\"\"\n",
    "        return torch.LongTensor([self.char_to_ix(i) for i in domain])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        domain = self.domain_to_ix(self.inputs[index])\n",
    "        if self.train:\n",
    "            target = torch.Tensor([self.labels[index]])\n",
    "            return domain, target\n",
    "        return domain\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DomainDataset('data', 'alexa_top_1m.csv').all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy, x_lens, y_lens\n",
    "\n",
    "def _get_train_test_data_loader(batch_size, data_dir, filename, n_records):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(data_dir, filename)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    \n",
    "    test_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    \n",
    "    test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "    return train_dl, test_dl\n",
    "\n",
    "def pad_collate_pred(batch):\n",
    "\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xx_pad, yy, x_lens, y_lens\n",
    "\n",
    "def _get_predict_loader(batch_size, data_dir, filename):\n",
    "    print(\"Getting test and train data loaders.\")\n",
    "    \n",
    "    dataset =  DomainDataset(data_dir, filename)\n",
    "    \n",
    "    predict_dl = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "    \n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 4.339s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestDGADataSet(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.n = 200\n",
    "        self.test_dga = DGADataset('data', 'alexa_top_1m.csv', self.n)\n",
    "\n",
    "    def test_length(self):\n",
    "        actual_len = self.test_dga.__len__()\n",
    "        self.assertEqual(actual_len, self.n * 2, \n",
    "                         \"DataSet is not the right length ({}), should be {}\".format(actual_len, 400)\n",
    "                        )\n",
    "\n",
    "    def test_labels(self):\n",
    "        expected_labels = ['benign', 'dga']\n",
    "        labels = self.test_dga.data_df['label'].unique().tolist()\n",
    "        self.assertEqual(labels, \n",
    "                         expected_labels, \n",
    "                         \"Incorrect labels ({}), expected {}\".format(labels, expected_labels)\n",
    "                        )\n",
    "        \n",
    "    def test_classes(self):\n",
    "        expected_classes = [0, 1]\n",
    "        classes = self.test_dga.data_df['class'].unique().tolist()\n",
    "        self.assertEqual(classes, \n",
    "                         expected_classes, \n",
    "                         \"Incorrect classes ({}), expected {})\".format(classes, expected_classes)\n",
    "                        )\n",
    "        \n",
    "    def test_index(self):\n",
    "        index = self.test_dga.data_df.index.tolist()\n",
    "        self.assertEqual(len(index), \n",
    "                         len(set(index)),\n",
    "                         \"Index contains duplicates ({}), expected {})\".format(len(set(index)), len(index)))\n",
    "\n",
    "    def test_getitem(self):\n",
    "        item = self.test_dga.__getitem__(0)\n",
    "        self.assertEqual(len(item), \n",
    "                         2,\n",
    "                         \"Get item output is wrong length ({}), expected {})\".format(len(item), 2))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "class DGAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Estimator for generating sequences of target variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features=65, hidden_dim=12, n_layers=2, output_dim=1, embedding_dim=5, batch_size=10):\n",
    "        super(DGAClassifier, self).__init__()\n",
    "\n",
    "        # Variables\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(input_features, embedding_dim)\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, n_layers, dropout=0.3, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def init_hidden(self):\n",
    "#         \"\"\"\n",
    "#         Initialize the hidden and cell states of the LSTM with zeros.\n",
    "#         \"\"\"\n",
    "#         return torch.zeros(self.hidden_layers, self.batch_size, self.hidden_dim)\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on batch of tracks.\n",
    "        \"\"\"\n",
    "        # x: (batch_size, longest_sequence, embedding) i.e. 10, 32, 5\n",
    "        # hidden size: (hidden_layers, batch_size, hidden_dim) i.e. 2, 10, 30\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        # x_embed: (batch_size, longest_sequence, 1?, embedding_size)\n",
    "        embed_x = self.embedding(x)\n",
    "        \n",
    "        x_packed = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        output_packed, hidden_state = self.rnn(x_packed)\n",
    "        \n",
    "        output_padded, lengths = pad_packed_sequence(output_packed, batch_first=False)\n",
    "        \n",
    "        output = output_padded.view(batch_size*seq_len, self.hidden_dim)\n",
    "        \n",
    "        adjusted_lengths = [(l-1)*batch_size + i for i, l in enumerate(lengths)]\n",
    "        \n",
    "        lengthTensor = torch.tensor(adjusted_lengths, dtype=torch.int64)\n",
    "        \n",
    "        output = output.index_select(0, lengthTensor)\n",
    "        \n",
    "        output = output.view(batch_size, self.hidden_dim)\n",
    "        \n",
    "        output = self.sigmoid(self.fc(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed, cuda=False):\n",
    "    # Set the random seed manually for reproducibility.\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# TO:DO Figure out splunk query\n",
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "# def stage(name):\n",
    "#     with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "#         df = pd.read_csv(f)\n",
    "#     with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "#         param = json.load(f)\n",
    "#     return df, param\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "data_dir = 'data'\n",
    "filename = 'alexa_top_1m.csv'\n",
    "n_records = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "{'feature_variables': ['domain'], 'target_variables': ['benign', 'dga']}\n"
     ]
    }
   ],
   "source": [
    "# # THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "train, test = _get_train_test_data_loader(batch_size, data_dir, filename, n_records)# print(df.describe())\n",
    "# print(df)\n",
    "param = {'feature_variables': ['domain'], 'target_variables': ['benign', 'dga']}\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7fc9e77db6a0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "def init(df, param):\n",
    "#     mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\n",
    "\n",
    "    \n",
    "    input_size = 67\n",
    "    hidden_dim = 30\n",
    "    num_classes = 1\n",
    "    learning_rate = 0.001\n",
    "    embedding_dim = 5\n",
    "    n_layers = 2\n",
    "\n",
    "    model = {\n",
    "        \"input_size\": input_size,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"learning_rate\": learning_rate,\n",
    "#         \"mapping\": mapping,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "    \n",
    "    model[\"train_loader\"], model[\"test_loader\"] = _get_train_test_data_loader(batch_size, data_dir, filename, n_records)\n",
    "\n",
    "    print(\"FIT build RNN model with input size \" + str(model[\"train_loader\"].dataset.__len__()))\n",
    "    print(\"FIT build model with target classes \" + str(num_classes))\n",
    "    \n",
    "#     if 'options' in param:\n",
    "#         if 'params' in param['options']:\n",
    "#             if 'epochs' in param['options']['params']:\n",
    "#                 model['num_epochs'] = int(param['options']['params']['epochs'])\n",
    "#             if 'batch_size' in param['options']['params']:\n",
    "#                 model['batch_size'] = int(param['options']['params']['batch_size'])\n",
    "\n",
    "    # RNN Layer\n",
    "    model['model'] = DGAClassifier(\n",
    "        input_features=model[\"input_size\"], \n",
    "        hidden_dim=model[\"hidden_dim\"], \n",
    "        n_layers=model[\"n_layers\"], \n",
    "        output_dim=model[\"num_classes\"],\n",
    "        embedding_dim=model[\"embedding_dim\"], \n",
    "        batch_size=model[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    model['criterion'] = torch.nn.BCELoss()\n",
    "    \n",
    "    model['optimizer'] = torch.optim.Adam(model['model'].parameters())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test and train data loaders.\n",
      "FIT build RNN model with input size 8000\n",
      "FIT build model with target classes 1\n",
      "{'input_size': 67, 'hidden_dim': 30, 'embedding_dim': 5, 'num_classes': 1, 'n_layers': 2, 'learning_rate': 0.001, 'num_epochs': 2, 'batch_size': 32, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x7fca01d82f98>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x7fca01d82da0>, 'model': DGAClassifier(\n",
      "  (embedding): Embedding(67, 5)\n",
      "  (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=30, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), 'criterion': BCELoss(), 'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "model = init(df,param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_target(label, label_to_ix):\n",
    "#     return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "# label_to_ix = {\"benign\": 0, \"dga\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_categories = [\"benign\", \"dga\"]\n",
    "# def categoryFromOutput(output):\n",
    "#     top_n, top_i = output.topk(1)\n",
    "#     category_i = top_i[0].item()\n",
    "#     return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_acc(y_pred, y_test):\n",
    "#     y_pred_tag = torch.round(y_pred)\n",
    "\n",
    "#     correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "#     acc = correct_results_sum/y_test.shape[0]\n",
    "#     acc = torch.round(acc * 100)\n",
    "    \n",
    "#     return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stats(accuracy, confusion_matrix, output, y):\n",
    "    output = torch.round(output).flatten()\n",
    "    equal = torch.eq(output, y)\n",
    "    correct = int(torch.sum(equal))\n",
    "    for j, i in zip(output, y):\n",
    "        confusion_matrix[int(i),int(j)]+=1\n",
    "\n",
    "    return accuracy + correct, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# train your model\n",
    "# returns a fit info json object and may modify the model object\n",
    "def fit(model, param):\n",
    "    \n",
    "    returns = {\"message\": \"model trained\"}\n",
    "\n",
    "    cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cpu\") if not cuda else torch.device(\"cuda:0\")\n",
    "    seed_everything(seed=1337, cuda=cuda)\n",
    "    \n",
    "    accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "    \n",
    "    for epoch in range(1, model['num_epochs'] + 1):\n",
    "\n",
    "        # Train\n",
    "        model['model'].train()\n",
    "        print(\"Train ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        t = time.time()\n",
    "\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate over dataset\n",
    "        for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"train_loader\"]):\n",
    "            \n",
    "            # Clear stored gradient\n",
    "            model[\"optimizer\"].zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            \n",
    "            loss = model['criterion'](output, torch.Tensor(y_padded).squeeze(0))\n",
    "\n",
    "            total_loss += float(loss)\n",
    "            \n",
    "            accuracy, confusion_matrix = update_stats(\n",
    "                accuracy, \n",
    "                confusion_matrix, \n",
    "                torch.Tensor(output), \n",
    "                torch.Tensor(y_padded)\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            model[\"optimizer\"].step()\n",
    "                            \n",
    "            print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"train_loader\"]), time.time() - t), end='\\r', flush=True)\n",
    "            t = time.time()\n",
    "            \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"train_loader\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"train_loader\"].dataset), accuracy / len(model[\"train_loader\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "        # Evaluate\n",
    "        model['model'].eval()\n",
    "        accuracy, confusion_matrix = 0, np.zeros((2, 2), dtype=int)\n",
    "        t = time.time()\n",
    "        total_loss = 0\n",
    "        print(\"Validation ({})\".format(epoch))\n",
    "        print(\"-\"*20)\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"test_loader\"]):\n",
    "                output =  model['model'](x_padded, x_lens)\n",
    "                total_loss += float(model['criterion'](output, torch.Tensor(y_padded).squeeze(0)))\n",
    "                accuracy, confusion_matrix = update_stats(\n",
    "                    accuracy, \n",
    "                    confusion_matrix, \n",
    "                    torch.Tensor(output), \n",
    "                    torch.Tensor(y_padded)\n",
    "                )\n",
    "                print(\"[Batch]: {}/{} in {:.5f} seconds\".format(batch_num, len(model[\"test_loader\"]), time.time() - t), end='\\r', flush=True)\n",
    "                t = time.time()\n",
    "                \n",
    "        print(\"\")\n",
    "        print(\"[Loss]: {:.5f}\".format(total_loss / len(model[\"test_loader\"])))\n",
    "        print(\"[Accuracy]: {}/{} : {:.3f}%\".format(\n",
    "            accuracy, len(model[\"test_loader\"].dataset), accuracy / len(model[\"test_loader\"].dataset) * 100))\n",
    "        print(confusion_matrix, \"\\n\")\n",
    "        \n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model['num_epochs']\n",
    "    returns['model_batch_size'] = model['batch_size']\n",
    "    returns['model_loss_acc'] = total_loss / len(model[\"train_loader\"])\n",
    "            \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (1)\n",
      "--------------------\n",
      "[Batch]: 249/250 in 0.03208 seconds\n",
      "[Loss]: 0.00405\n",
      "[Accuracy]: 7994/8000 : 99.925%\n",
      "[[3987    5]\n",
      " [   1 4007]] \n",
      "\n",
      "Validation (1)\n",
      "--------------------\n",
      "[Batch]: 62/63 in 0.00597 seconds\n",
      "[Loss]: 0.00404\n",
      "[Accuracy]: 1999/2000 : 99.950%\n",
      "[[1007    1]\n",
      " [   0  992]] \n",
      "\n",
      "Train (2)\n",
      "--------------------\n",
      "[Batch]: 249/250 in 0.06303 seconds\n",
      "[Loss]: 0.01950\n",
      "[Accuracy]: 7965/8000 : 99.562%\n",
      "[[3970   22]\n",
      " [  13 3995]] \n",
      "\n",
      "Validation (2)\n",
      "--------------------\n",
      "[Batch]: 62/63 in 0.01706 seconds\n",
      "[Loss]: 0.01838\n",
      "[Accuracy]: 1993/2000 : 99.650%\n",
      "[[1001    7]\n",
      " [   0  992]] \n",
      "\n",
      "{'message': 'model trained', 'model_epochs': 2, 'model_batch_size': 32, 'model_loss_acc': 0.0046313053680351}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(fit(model,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('benign', 'dga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # print images\n",
    "# # imshow(torchvision.utils.make_grid(images))\n",
    "# print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNEstimator()\n",
    "net.load_state_dict(rnn, strict=False)\n",
    "net.eval()\n",
    "\n",
    "dataiter = iter(testLoader)\n",
    "domains, labels, xx_lens, _ = dataiter.next()\n",
    "\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(len(labels))))\n",
    "\n",
    "def evaluate(domains):\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            input = domains\n",
    "            hidden = model['model'].init_hidden()\n",
    "            output = model['model'](input, xx_lens, hidden)[0]\n",
    "            y_hat = output[:, -1, :]\n",
    "            print(y_hat)\n",
    "            _, predicted = torch.max(y_hat, 1)\n",
    "            prediction = [classes[key] for key in predicted.numpy()]\n",
    "        \n",
    "evaluate(domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "#TO-DO: How does Splunk stage data for apply method. In a CSV?\n",
    "def apply(model, df, param):\n",
    "    \n",
    "    test_df = _get_train_data_loader(batch_size, data_dir, filename, 100)# print(df.describe())\n",
    "    \n",
    "    classes = {v: k for k, v in model['mapping'].items()}\n",
    "    \n",
    "    # Evaluate\n",
    "    model['model'].eval()\n",
    "    print(\"Validation ({})\".format(epoch))\n",
    "    print(\"-\"*20)\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (x_padded, y_padded, x_lens, y_lens) in enumerate(model[\"test_loader\"]):\n",
    "            output =  model['model'](x_padded, x_lens)\n",
    "            output = torch.round(output).flatten()\n",
    "    return batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = RNNEstimator()\n",
    "# net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# load(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get train data loader.\n",
      "torch.Size([10, 35, 2])\n",
      "tensor([[0.2936, 0.7276],\n",
      "        [0.0718, 0.9395],\n",
      "        [0.0216, 0.9853],\n",
      "        [0.0138, 0.9911],\n",
      "        [0.0142, 0.9907],\n",
      "        [0.0156, 0.9898],\n",
      "        [0.0139, 0.9908],\n",
      "        [0.0146, 0.9904],\n",
      "        [0.0175, 0.9885],\n",
      "        [0.0152, 0.9901],\n",
      "        [0.0131, 0.9914],\n",
      "        [0.0144, 0.9902],\n",
      "        [0.0159, 0.9896],\n",
      "        [0.0136, 0.9912],\n",
      "        [0.0135, 0.9913],\n",
      "        [0.0136, 0.9912],\n",
      "        [0.0159, 0.9897],\n",
      "        [0.0143, 0.9907],\n",
      "        [0.0161, 0.9894],\n",
      "        [0.0144, 0.9905],\n",
      "        [0.0146, 0.9906],\n",
      "        [0.0135, 0.9913],\n",
      "        [0.0134, 0.9912],\n",
      "        [0.0147, 0.9904],\n",
      "        [0.0140, 0.9909],\n",
      "        [0.0143, 0.9906],\n",
      "        [0.0145, 0.9903],\n",
      "        [0.0154, 0.9902],\n",
      "        [0.0125, 0.9919],\n",
      "        [0.5005, 0.5264],\n",
      "        [0.5005, 0.5264],\n",
      "        [0.5005, 0.5264],\n",
      "        [0.5005, 0.5264],\n",
      "        [0.5005, 0.5264],\n",
      "        [0.5005, 0.5264]])\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",
    "print(apply(model,df,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNNEstimator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_size': 66,\n",
       " 'hidden_dim': 30,\n",
       " 'embedding_dim': 5,\n",
       " 'num_classes': 2,\n",
       " 'n_layers': 2,\n",
       " 'learning_rate': 0.001,\n",
       " 'mapping': {0: 0, 1: 1},\n",
       " 'num_epochs': 5,\n",
       " 'batch_size': 10,\n",
       " 'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7fd5cc17cf98>,\n",
       " 'model': RNNEstimator(\n",
       "   (embedding): Embedding(66, 5)\n",
       "   (rnn): RNN(5, 30, num_layers=2, batch_first=True, dropout=0.3)\n",
       "   (fc): Linear(in_features=30, out_features=2, bias=True)\n",
       " ),\n",
       " 'criterion': CrossEntropyLoss(),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     eps: 1e-08\n",
       "     lr: 0.001\n",
       "     weight_decay: 0\n",
       " )}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(model, 'dga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = load('dga')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",
    "    if model is not None:\n",
    "        if 'model' in model:\n",
    "            returns[\"summary\"] = str(model)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing your fit, apply, save and load you can train your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or apply your model:<br>\n",
    "| makeresults count=10<br>\n",
    "| streamstats c as i<br>\n",
    "| eval s = i%3<br>\n",
    "| eval feature_{s}=0<br>\n",
    "| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]<br>\n",
    "| apply barebone_model as the_meaning_of_life"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
